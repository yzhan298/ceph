; generated by vstart.sh on Fri Jul 20 16:11:56 EDT 2018
[client.vstart.sh]
        num mon = 1
        num osd = 1
        num mds = 0
        num mgr = 0
        num rgw = 1

[global]
        fsid = 1140b0c0-4372-4742-875a-7f5254f4523c
        osd failsafe full ratio = .99
        mon osd full ratio = .99
        mon osd nearfull ratio = .99
        mon osd backfillfull ratio = .99
        erasure code dir = /home2/ivancich/ceph-work2/ceph/build/lib
        plugin dir = /home2/ivancich/ceph-work2/ceph/build/lib
        filestore fd cache size = 32
        run dir = /home2/ivancich/ceph-work2/ceph/build/out
        enable experimental unrecoverable data corrupting features = *
	osd_crush_chooseleaf_type = 0

        lockdep = true
[client]
        keyring = /home2/ivancich/ceph-work2/ceph/build/keyring
        log file = /home2/ivancich/ceph-work2/ceph/build/out/$name.$pid.log
        admin socket = /tmp/ceph-asok.kCpq8z/$name.$pid.asok

[client.rgw]
        rgw frontends = civetweb port=8000
        ; needed for s3tests
        rgw crypt s3 kms encryption keys = testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo= testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
        rgw crypt require ssl = false
        rgw lc debug interval = 10

[mds]

        log file = /home2/ivancich/ceph-work2/ceph/build/out/$name.log
        admin socket = /tmp/ceph-asok.kCpq8z/$name.asok
        chdir = ""
        pid file = /home2/ivancich/ceph-work2/ceph/build/out/$name.pid
        heartbeat file = /home2/ivancich/ceph-work2/ceph/build/out/$name.heartbeat

        mds data = /home2/ivancich/ceph-work2/ceph/build/dev/mds.$id
        mds root ino uid = 1000
        mds root ino gid = 1000

[mgr]
        mgr data = /home2/ivancich/ceph-work2/ceph/build/dev/mgr.$id
        mgr module path = /home2/ivancich/ceph-work2/ceph/src/pybind/mgr

        log file = /home2/ivancich/ceph-work2/ceph/build/out/$name.log
        admin socket = /tmp/ceph-asok.kCpq8z/$name.asok
        chdir = ""
        pid file = /home2/ivancich/ceph-work2/ceph/build/out/$name.pid
        heartbeat file = /home2/ivancich/ceph-work2/ceph/build/out/$name.heartbeat


[osd]

        log file = /home2/ivancich/ceph-work2/ceph/build/out/$name.log
        admin socket = /tmp/ceph-asok.kCpq8z/$name.asok
        chdir = ""
        pid file = /home2/ivancich/ceph-work2/ceph/build/out/$name.pid
        heartbeat file = /home2/ivancich/ceph-work2/ceph/build/out/$name.heartbeat

        osd_check_max_object_name_len_on_startup = false
        osd data = /home2/ivancich/ceph-work2/ceph/build/dev/osd$id
        osd journal = /home2/ivancich/ceph-work2/ceph/build/dev/osd$id/journal
        osd journal size = 100
        osd class tmp = out
        osd class dir = /home2/ivancich/ceph-work2/ceph/build/lib
        osd class load list = *
        osd class default list = *

        filestore wbthrottle xfs ios start flusher = 10
        filestore wbthrottle xfs ios hard limit = 20
        filestore wbthrottle xfs inodes hard limit = 30
        filestore wbthrottle btrfs ios start flusher = 10
        filestore wbthrottle btrfs ios hard limit = 20
        filestore wbthrottle btrfs inodes hard limit = 30
        bluestore fsck on mount = true
        bluestore block create = true
	bluestore block db path = /home2/ivancich/ceph-work2/ceph/build/dev/osd$id/block.db.file
        bluestore block db size = 67108864
        bluestore block db create = true
	bluestore block wal path = /home2/ivancich/ceph-work2/ceph/build/dev/osd$id/block.wal.file
        bluestore block wal size = 1048576000
        bluestore block wal create = true

	; **************** eric additions ****************
	osd op num shards = 2
	osd op num threads per shard = 2
        osd throttle = 1
	osd client message cap = 500
	osd client message size cap = 1048576000
	debug osd = 10

        ; kstore
        kstore fsck on mount = true
        osd objectstore = bluestore
        osd max object name len = 460
        osd max object namespace len = 64

[mon]
        mgr initial modules = dashboard restful status balancer iostat

        log file = /home2/ivancich/ceph-work2/ceph/build/out/$name.log
        admin socket = /tmp/ceph-asok.kCpq8z/$name.asok
        chdir = ""
        pid file = /home2/ivancich/ceph-work2/ceph/build/out/$name.pid
        heartbeat file = /home2/ivancich/ceph-work2/ceph/build/out/$name.heartbeat


        debug mon = 20
        debug paxos = 20
        debug auth = 20
	debug mgrc = 20
        debug ms = 1

        mon cluster log file = /home2/ivancich/ceph-work2/ceph/build/out/cluster.mon.$id.log
        osd pool default erasure code profile = plugin=jerasure technique=reed_sol_van k=2 m=1 crush-failure-domain=osd
[mon.a]
        host = sleepy
        mon data = /home2/ivancich/ceph-work2/ceph/build/dev/mon.a
        mon addr = 127.0.0.1:40025
[osd.0]
        host = sleepy
